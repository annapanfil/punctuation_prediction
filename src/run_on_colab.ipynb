{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1e9uIAaw-A"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXr1UEpEiXJj",
        "outputId": "3ff052d7-b8cf-4c5f-ad52-3f2558f6c029"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFtO2Y6OiYSI",
        "outputId": "70f16229-8130-470d-a99a-51e9dab6434d"
      },
      "outputs": [],
      "source": [
        "# clone repo via ssh\n",
        "\n",
        "!cp /content/drive/MyDrive/.ssh /root/  -r\n",
        "!chmod 644 /root/.ssh/known_hosts\n",
        "!chmod 600 /root/.ssh/id_rsa\n",
        "!ssh -T git@github.com\n",
        "\n",
        "!git clone -q --recurse-submodules git@github.com:annapanfil/punctuation_prediction.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwKAZr6nk2cE",
        "outputId": "2b485294-e5aa-49ae-ccfc-6603f9ccefc5"
      },
      "outputs": [],
      "source": [
        "%cd punctuation_prediction/\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbvAr9FkkvqR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"annapanfil\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"f3ab7a17321dd66221aae96b7dbc9f43434a812d\"\n",
        "# os.environ['TOKENIZERS_PARALLELISM'] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWRcao1a6Ko"
      },
      "source": [
        "# Conduct experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 src/dataset_creation.py --newlines=false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4RRyDpnqqLG"
      },
      "outputs": [],
      "source": [
        "# models\n",
        "sh = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "# Function to run training\n",
        "run_training() {\n",
        "    pretrained_model=$1\n",
        "    language=$2\n",
        "    epochs=$3\n",
        "    command=\"python src/train.py --pretrained-model=${pretrained_model} --language=${language} --epoch=${epochs} --cuda=True\"\n",
        "    echo \"Running command: $command\"\n",
        "    $command\n",
        "}\n",
        "\n",
        "# List of different models to try\n",
        "models=(\"herbert-base\" \"bert-multiling-uncased\" \"roberta-multiling\" \"polish-roberta\")\n",
        "language=\"polish\"\n",
        "epochs=10\n",
        "\n",
        "# Run training for each model\n",
        "for model in \"${models[@]}\"\n",
        "do\n",
        "    run_training \"$model\" \"$language\" \"$epochs\"\n",
        "done\n",
        "\"\"\"\n",
        "with open('script.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "!bash script.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsOhCH5Zivm8"
      },
      "outputs": [],
      "source": [
        "# !python src/train.py --cuda=True --pretrained-model=bert-base-uncased --freeze-bert=False --lstm-dim=-1 --language=english --seed=1 --lr=5e-6 --epoch=8 --use-crf=False --augment-type=all  --augment-rate=0.15 --alpha-sub=0.4 --alpha-del=0.4 --data-path=data --save-path=out\n",
        "# !python src/train.py --pretrained-model=herbert-base --language=polish --epoch=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZSUGApDnqlM"
      },
      "outputs": [],
      "source": [
        "# !python src/inference.py --pretrained-model=bert-base-uncased --weight-path=out/weights.pt --language=en --in-file=data/test_en.txt --out-file=data/test_en_out.txt\n",
        "# !python src/inference.py --pretrained-model=herbert-base --weight-path=out/weights.pt --language=pl --in-file=data/test_pl.txt --out-file=data/test_pl_out.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM and freezing BERT\n",
        "\n",
        "!python src/train.py --freeze-bert=False --lstm=bi --name=\"Bi, no freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=10 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "!python src/train.py --freeze-bert=True --lstm=bi --name=\"Bi, freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=20 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "\n",
        "!python src/train.py --freeze-bert=False --lstm=uni --name=\"Uni, no freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=10 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "!python src/train.py --freeze-bert=True --lstm=uni --name=\"Uni, freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=20 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "\n",
        "!python src/train.py --freeze-bert=False --lstm=none --name=\"No LSTM, no freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=10 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "!python src/train.py --freeze-bert=True --lstm=none --name=\"No LSTM, freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=20 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "sh = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "# Function to run training\n",
        "run_training() {\n",
        "    lr=$1\n",
        "    name=\"lr=$lr\"\n",
        "    command=\"python src/train.py --name=${name} --lr=${lr} --epoch=10 --freeze-bert=False --lstm=none --language=polish --cuda=True --pretrained-model=polish-roberta --log=true --data-variation=allpunct --experiment-name=hyperparameters\"\n",
        "    echo \"Running command: $command\"\n",
        "    $command\n",
        "}\n",
        "\n",
        "# List of different models to try\n",
        "lrs=(1e-1 1e-2 1e-3 1e-4 1e-5)\n",
        "\n",
        "# Run training for each model\n",
        "for lr in \"${lrs[@]}\"\n",
        "do\n",
        "    run_training \"$lr\"\n",
        "done\n",
        "\"\"\"\n",
        "with open('script.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "!bash script.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python src/train.py --name=\"No LSTM, no freeze\" --experiment-name=\"LSTM and freezing BERT\" --use-lora=True --batch-size=16 --epoch=10 --sequence-length=128 --lr=1e-5 --log=true --freeze-bert=False --lstm=none  --data-variation=allpunct --language=polish --cuda=True --pretrained-model=polish-roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python src/train.py --name=\"LORA r=128 batch 32\" --experiment-name=\"hyperparameters\" --use-lora=True --batch-size=32 --epoch=10 --sequence-length=128 --lr=1e-5 --freeze-bert=False --lstm=none --log=true --data-variation=allpunct --pretrained-model=polish-roberta --language=polish --cuda=True"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
