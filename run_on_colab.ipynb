{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1e9uIAaw-A"
      },
      "source": [
        "# Prepare environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXr1UEpEiXJj",
        "outputId": "3ff052d7-b8cf-4c5f-ad52-3f2558f6c029"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFtO2Y6OiYSI",
        "outputId": "70f16229-8130-470d-a99a-51e9dab6434d"
      },
      "outputs": [],
      "source": [
        "# clone repo via ssh\n",
        "\n",
        "!cp /content/drive/MyDrive/.ssh /root/  -r\n",
        "!chmod 644 /root/.ssh/known_hosts\n",
        "!chmod 600 /root/.ssh/id_rsa\n",
        "!ssh -T git@github.com\n",
        "\n",
        "!git clone -q --recurse-submodules git@github.com:annapanfil/punctuation_prediction.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwKAZr6nk2cE",
        "outputId": "2b485294-e5aa-49ae-ccfc-6603f9ccefc5"
      },
      "outputs": [],
      "source": [
        "%cd punctuation_prediction/\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbvAr9FkkvqR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = \"annapanfil\"\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = \"f3ab7a17321dd66221aae96b7dbc9f43434a812d\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWRcao1a6Ko"
      },
      "source": [
        "# Conduct experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 src/dataset_creation.py --newlines=false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4RRyDpnqqLG"
      },
      "outputs": [],
      "source": [
        "# models\n",
        "sh = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "# Function to run training\n",
        "run_training() {\n",
        "    pretrained_model=$1\n",
        "    language=$2\n",
        "    epochs=$3\n",
        "    command=\"python src/train.py --pretrained-model=${pretrained_model} --language=${language} --epoch=${epochs} --cuda=True\"\n",
        "    echo \"Running command: $command\"\n",
        "    $command\n",
        "}\n",
        "\n",
        "# List of different models to try\n",
        "models=(\"herbert-base\" \"bert-multiling-uncased\" \"roberta-multiling\" \"polish-roberta\")\n",
        "language=\"polish\"\n",
        "epochs=10\n",
        "\n",
        "# Run training for each model\n",
        "for model in \"${models[@]}\"\n",
        "do\n",
        "    run_training \"$model\" \"$language\" \"$epochs\"\n",
        "done\n",
        "\"\"\"\n",
        "with open('script.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "!bash script.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsOhCH5Zivm8"
      },
      "outputs": [],
      "source": [
        "# !python src/train.py --cuda=True --pretrained-model=bert-base-uncased --freeze-bert=False --lstm-dim=-1 --language=english --seed=1 --lr=5e-6 --epoch=8 --use-crf=False --augment-type=all  --augment-rate=0.15 --alpha-sub=0.4 --alpha-del=0.4 --data-path=data --save-path=out\n",
        "# !python src/train.py --pretrained-model=herbert-base --language=polish --epoch=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZSUGApDnqlM"
      },
      "outputs": [],
      "source": [
        "# !python src/inference.py --pretrained-model=bert-base-uncased --weight-path=out/weights.pt --language=en --in-file=data/test_en.txt --out-file=data/test_en_out.txt\n",
        "# !python src/inference.py --pretrained-model=herbert-base --weight-path=out/weights.pt --language=pl --in-file=data/test_pl.txt --out-file=data/test_pl_out.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM and freezing BERT\n",
        "\n",
        "!python src/train.py --freeze-bert=False --lstm=bi --name=\"Bi, no freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=10 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "!python src/train.py --freeze-bert=True --lstm=bi --name=\"Bi, freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=20 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "\n",
        "!python src/train.py --freeze-bert=False --lstm=uni --name=\"Uni, no freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=10 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "!python src/train.py --freeze-bert=True --lstm=uni --name=\"Uni, freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=20 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "\n",
        "!python src/train.py --freeze-bert=False --lstm=none --name=\"No LSTM, no freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=10 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\"\n",
        "!python src/train.py --freeze-bert=True --lstm=none --name=\"No LSTM, freeze\" --language=polish --cuda=True --pretrained-model=polish-roberta --epoch=20 --log=true --data-variation=allpunct --experiment-name=\"LSTM and freezing BERT\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running command: python src/train.py --name=lr 1e-1 --lr=1e-1 --epoch=10 --freeze-bert=False --lstm=none --language=polish --cuda=True --pretrained-model=polish-roberta --log=true --data-variation=allpunct --experiment-name=hyperparameters\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "usage: train.py [-h] [--name NAME] [--experiment-name EXPERIMENT_NAME]\n",
            "                [--data-path DATA_PATH] [--data-variation DATA_VARIATION]\n",
            "                [--language LANGUAGE] [--cuda CUDA] [--seed SEED]\n",
            "                [--pretrained-model PRETRAINED_MODEL]\n",
            "                [--freeze-bert FREEZE_BERT] [--lstm LSTM]\n",
            "                [--lstm-dim LSTM_DIM] [--use-crf USE_CRF]\n",
            "                [--sequence-length SEQUENCE_LENGTH]\n",
            "                [--augment-rate AUGMENT_RATE] [--augment-type AUGMENT_TYPE]\n",
            "                [--sub-style SUB_STYLE] [--alpha-sub ALPHA_SUB]\n",
            "                [--alpha-del ALPHA_DEL] [--lr LR] [--decay DECAY]\n",
            "                [--gradient-clip GRADIENT_CLIP] [--batch-size BATCH_SIZE]\n",
            "                [--epoch EPOCH] [--log LOG]\n",
            "train.py: error: unrecognized arguments: 1e-1\n",
            "Running command: python src/train.py --name=lr 1e-2 --lr=1e-2 --epoch=10 --freeze-bert=False --lstm=none --language=polish --cuda=True --pretrained-model=polish-roberta --log=true --data-variation=allpunct --experiment-name=hyperparameters\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/mnt/dane/projekty/studia/punctuation_prediction/src/train.py\", line 2, in <module>\n",
            "    import torch\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/__init__.py\", line 1919, in <module>\n",
            "    from . import _meta_registrations\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_meta_registrations.py\", line 9, in <module>\n",
            "    from torch._decomp import (\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_decomp/__init__.py\", line 244, in <module>\n",
            "    import torch._decomp.decompositions\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_decomp/decompositions.py\", line 11, in <module>\n",
            "    import torch._prims as prims\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_prims/__init__.py\", line 3031, in <module>\n",
            "    register_debug_prims()\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_prims/debug_prims.py\", line 40, in register_debug_prims\n",
            "    @load_tensor.impl_factory()\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_custom_op/impl.py\", line 333, in inner\n",
            "    self._register_impl(\"factory\", f)\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/site-packages/torch/_custom_op/impl.py\", line 223, in _register_impl\n",
            "    frame = inspect.getframeinfo(sys._getframe(stacklevel))\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/inspect.py\", line 1711, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/inspect.py\", line 1087, in findsource\n",
            "    module = getmodule(object, file)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/inspect.py\", line 1010, in getmodule\n",
            "    f = getabsfile(module)\n",
            "        ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/inspect.py\", line 979, in getabsfile\n",
            "    _filename = getsourcefile(object) or getfile(object)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/inspect.py\", line 955, in getsourcefile\n",
            "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/mnt/dane/.conda/envs/mgr/lib/python3.12/inspect.py\", line 955, in <genexpr>\n",
            "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
            "\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "\n",
        "sh = \"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "# Function to run training\n",
        "run_training() {\n",
        "    lr=$1\n",
        "    name=\"lr $lr\"\n",
        "    command=\"python src/train.py --name=${name} --lr=${lr} --epoch=10 --freeze-bert=False --lstm=none --language=polish --cuda=True --pretrained-model=polish-roberta --log=true --data-variation=allpunct --experiment-name=hyperparameters\"\n",
        "    echo \"Running command: $command\"\n",
        "    $command\n",
        "}\n",
        "\n",
        "# List of different models to try\n",
        "lrs=(1e-1 1e-2 1e-3 1e-4 1e-5)\n",
        "\n",
        "# Run training for each model\n",
        "for lr in \"${lrs[@]}\"\n",
        "do\n",
        "    run_training \"$lr\"\n",
        "done\n",
        "\"\"\"\n",
        "with open('script.sh', 'w') as file:\n",
        "  file.write(sh)\n",
        "\n",
        "!bash script.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
